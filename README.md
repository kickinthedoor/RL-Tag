# RL-Tag
This project is a multi-agent game of catch. The objective is, as traditionally, for the hunter to catch the prey and for the prey to try and escape. 

# Quickstart guide
NOTE! Setting up is required to use GPU, which is needed to run the program.<br/><br/>
Download the repository, optionally create a virtual env using: 'python -m venv venv', activate the venv with: 'venv\Scripts\activate', after which you install the requirements with: 'pip install -r requirements.txt'. Once that is done, you can run the training by simply running 'start.bat' and running 'py scripts\train.py'. 

# Design choices
The algorithm chosen for the model is PPO (Proximal policy optimization), due to its fit for MARL (Multi-agent reinforcement learning), stable training and its fit to both discrete and continuous environments.
Multi-agent tasks introduce an additional layer of interdependencies, where metrics used to analyze single-agent training can't be used the same way. Even more so, when the training consists of a zero-sum game, where  the reward of one agent is mirrored in proportion as a penalty for the other. <br/><br/>
Usual metrics, such as loss, KL divergence, episode reward and policy entropy need to be customized porperly chosen in a way that useful information can be extracted.
Some such metrics could be hunter win rate, prey win rate and average game length. Policy entropy can be used to discern, whether the policy of either agent drops too fast. KL divergence needs to be viewed in relation to each agent, as too high KL indicates instability and near 0 KL indicates stagnation.
The training progress can also be visualized by periodically rendering a simulated game based on the current training progress.

# Ideas and considerations
The map currently is hard coded to be a square grid with a rectangular obstacle in the center. One could improve the generalization ability of the agents by randomly generating a map with obstacles. Training would take longer, of course, but the model itself would learn to generalize better than on a static environment.
Another change that could be made is changing the spawn points of the agents early on to be constant, and later on changing them to random spawns. The idea being that the agents might potentially form strategies early on and later the random spawns could help the generalize. <br/><br/>
As it stands, agents can see the entire environment, including the opponent. A potential change could be to limit the vision to some range. Maybe the hunter and prey could even have unique properties, such as the hunter having narrow, predatory vision, while the prey would have a wider view. The agents could also have different abilities to move. For now both share the same simple actions of moving up, down, left and right.
Next would also be the possibility of testing another algorithm, such as A3C, which would need heavier parameter tuning to properly train the model.


# Training progress
After 2 million iterations, the strategies of the agents look as follows.<br/>
![https://media3.giphy.com/media/aUovxH8Vf9qDu/giphy.gif](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExMHF5MXgzbWVwcTFpdWVzMXZjaTljZHV2OXR3aWhrdWVmNmE4ZGc2MiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/GdAX4zqp0lJjdXRb3f/giphy.gif)
<br/><br/>
## Average game length
![avg_game_length](https://github.com/user-attachments/assets/c87f3d23-a3e8-46cc-b46a-3273a27ae16c)
## Policy entropies
![hunter_policy_entropy](https://github.com/user-attachments/assets/a78b69fd-cce7-4b06-aa7b-1f188f02c610)

![prey_policy_entropy](https://github.com/user-attachments/assets/d023811d-dbee-4b1e-be84-59c015ee8ad0)
## Policy KL
![hunter_policy_kl](https://github.com/user-attachments/assets/a4be3193-7b66-4699-a9ff-416105b40631)

![prey_policy_kl](https://github.com/user-attachments/assets/42d11cd2-17b6-4fe3-813a-625b1bec2c50)
## Win rates
![hunter_win_rate](https://github.com/user-attachments/assets/a80785b9-a4e6-4a33-8547-49a8adbeb722)

![prey_win_rate](https://github.com/user-attachments/assets/60e43a3c-861d-4cde-aed9-ddae65a1e0e8)
## Total rewards
![hunter_total_reward](https://github.com/user-attachments/assets/41db0ba1-4d4f-40e2-ac22-8c19ed3e8130)

![prey_total_reward](https://github.com/user-attachments/assets/d26bd846-6347-476e-a1e5-1bcee44b80d5)
## Reward per step
![hunter_reward_per_step](https://github.com/user-attachments/assets/1ccea9ef-6c33-4118-a359-e4127dd01a10)

![prey_reward_per_step](https://github.com/user-attachments/assets/4d9aad44-701c-4c6f-8e4d-43b1b66cb8ea)



# Directories

## ðŸ“‚ env/
Core environment logic and wrappers (e.g., obstacle handling, observation encoding, and multi-agent interfaces).

```
env/
â”œâ”€â”€ generate_map.py
â”œâ”€â”€ helpers.py
â”œâ”€â”€ observation_encoder.py
â”œâ”€â”€ rllib_wrapper.py
â”œâ”€â”€ tag_env.py
```

## ðŸ“‚ models/
Additional required models.
```
models/
â”œâ”€â”€ action_mask_model.py
```

## ðŸ“‚ config/
Holds agent and training configuration logic.

```
config/
â”œâ”€â”€ agent_config.py
â”œâ”€â”€ config.py
```

## ðŸ“‚ scripts/
Script entry point for training and running the project.

```
scripts/
â”œâ”€â”€ train.py
```

## ðŸ“‚ utils/
Reusable utilities such as logging, checkpoint management, callbacks, and GIF generation.

```
utils/
â”œâ”€â”€ callbacks.py
â”œâ”€â”€ checkpoint_utils.py
â”œâ”€â”€ gif_utils.py
â”œâ”€â”€ logger.py
```


# Detailed project structure


```
env/
â”œâ”€â”€ generate_map.py
â”‚   â”œâ”€â”€ generate_obstacles()
â”‚   â””â”€â”€ random_position()
â”œâ”€â”€ helpers.py
â”‚   â”œâ”€â”€ calculate_distance_with_obstacles()
â”‚   â””â”€â”€ is_obstacle_between()
â”œâ”€â”€ observation_encoder.py
â”‚   â”œâ”€â”€ one_hot_encode()
â”‚   â””â”€â”€ normalize()
â”œâ”€â”€ tag_env.py
â”‚   â””â”€â”€ class TagEnv
â”‚       â”œâ”€â”€ __init__()
â”‚       â”œâ”€â”€ step()
â”‚       â”œâ”€â”€ reset()
â”‚       â”œâ”€â”€ observe()
â”‚       â”œâ”€â”€ _was_done_step()
â”‚       â”œâ”€â”€ get_valid_actions()
â”‚       â”œâ”€â”€ render()
â”‚       â”œâ”€â”€ render_frame()
â”‚       â”œâ”€â”€ close()
â”‚       â”œâ”€â”€ observation_space()
â”‚       â”œâ”€â”€ action_space()
â”‚       â”œâ”€â”€ calculate_rewards()
â”‚       â”œâ”€â”€ check_terminations()
â”‚       â””â”€â”€ check_truncations()

models/
â”œâ”€â”€ action_mask_model.py
â”‚   â””â”€â”€ class TorchActionMaskModel
â”‚       â”œâ”€â”€ __init__()
â”‚       â”œâ”€â”€ forward()
â”‚       â”œâ”€â”€ value_function()

config/
â”œâ”€â”€ agent_config.py
â”‚   â””â”€â”€ build_multiagent_config()
â”œâ”€â”€ config.py

scripts/
â”œâ”€â”€ train.py
â”‚   â””â”€â”€ main()

utils/
â”œâ”€â”€ callbacks.py
â”‚   â””â”€â”€ class CustomCallbacks
â”‚       â”œâ”€â”€ __init__()
â”‚       â”œâ”€â”€ on_episode_end()
â”‚       â””â”€â”€ on_train_result()
â”œâ”€â”€ checkpoint_utils.py
â”‚   â”œâ”€â”€ create_checkpoint_dir()
â”‚   â””â”€â”€ get_best_checkpoint()
â”œâ”€â”€ gif_utils.py
â”‚   â”œâ”€â”€ generate_gif_from_checkpoint()
â”‚   â””â”€â”€ load_snapshot_paths()
â””â”€â”€ logger.py
    â””â”€â”€ log()
```


